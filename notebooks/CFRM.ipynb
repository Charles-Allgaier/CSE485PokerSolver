{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter-Factual Regret Minimization\n",
    "\n",
    "CFRM is an algorithm for developing a **Nash-Equilibrium** in imperfect-information games.  At a high-level, CFRM works by playing against itself and iterativly updating its strategy until it converges on a best strategy.\n",
    "\n",
    "#### Nash Equilibrium\n",
    "A Nash Equilibrium strategy is a strategy which is **unexploitable** in the long run.  This means that in the worst-case we will tie our opponent.  In a simple game like tic-tac-toe, this will likely lead to a tie, but poker is so complex that no human player plays a nash-equilibrium strategy so we will be able to profit.  It's important to note that this works by calculating a perfect strategy and does not involve exploiting weaknesses in the opponents play.\n",
    "\n",
    "Unlike in perfect information games like chess, in imperfect-information games like poker, the true state of the game is not known because we cannot see the other player cards.  Because of this, CFRM does not work on game states and instead works on **Information Sets**\n",
    "\n",
    "#### Information Sets\n",
    "An **Information set** is a unique strategic situation that encompasses all known information.  In our case this is...\n",
    " - Our two cards\n",
    " - The cards on the table\n",
    " - The betting history (has the other just checked, what did he do on the flop, ect.)\n",
    " \n",
    "At each information set we will calculate a strategy relating to the frequency we will take an available action.  For example...\n",
    " - Bet \\$50: 10%\n",
    " - Bet \\$100: 80%\n",
    " - Check: 10%\n",
    " \n",
    "Our **current strategy** for an information set is usually stored in a list of **regrets**\n",
    "\n",
    "#### Regrets\n",
    "A regret is a value representing how benificial an action was.  An action with *high-positive* regret should be take over an action an with *negative regret*.  We update our regrets (our current strategy) using an algorithm known as **regret-matching**.\n",
    "\n",
    "#### Regret Matching\n",
    "\n",
    "Regret matching works by first calculating the utility of every possible action. For example, if our opponent has throw *ROCK*, our action utilities would be as follows\n",
    "\n",
    "[ PAPER = 1, ROCK = 0, SCISSORS = -1 ]\n",
    "\n",
    "Next, we calculate the utility of the action we chose, lets say *PAPER*.  Since PAPER loses to ROCK, our utility would be -1.\n",
    "\n",
    "Now, we're ready to update our strategy.  To do this we add to each regret the difference between our action utility and the utility of the corresponding action.  What this does, is say for each action \"How much better off if I'd taken this action instead of the one I did?\".  If the answer is better, it adds regret to that action so it will be taken more often in the future.  If the answer is worse, it subtracts regret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regret Matching Example: Rock, Paper, Scissors\n",
    "\n",
    "To demonstrate how regret matching-works, we'll use rock, paper, scissors as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n",
      "[0.34996857 0.24128949 0.40874195]\n",
      "[0.33494994 0.34510389 0.31994617]\n",
      "[0.36232344 0.32127017 0.3164064 ]\n",
      "[0.30514789 0.35707226 0.33777984]\n",
      "[0.33911095 0.29119312 0.36969594]\n",
      "[0.31003695 0.34676921 0.34319384]\n",
      "[0.33033803 0.29976849 0.36989348]\n",
      "[0.30540924 0.34343958 0.35115119]\n",
      "[0.32132679 0.31043042 0.36824279]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# We have three actions to choose from\n",
    "ROCK, SCISSORS, PAPER = 0, 1, 2\n",
    "ACTIONS = [ROCK, SCISSORS, PAPER]\n",
    "# ROCK > SCISSORS, SCISSORS > PAPER, PAPER > ROCK\n",
    "\n",
    "def get_utility(my_action, opp_action):\n",
    "    if my_action == opp_action: # tie\n",
    "        return 0\n",
    "    if (my_action+1)%3 == opp_action: # we win!\n",
    "        return 1\n",
    "    else: # lose\n",
    "        return -1\n",
    "\n",
    "# Let's set our initial strategy to always throw PAPER\n",
    "regrets = [0, 1, 0]\n",
    "# And set our avg_strategy to zeros\n",
    "strategy_sum = np.zeros(3)\n",
    "\n",
    "opp_strategy = [0.5, 0.0, 0.5]\n",
    "\n",
    "# Get strategy through regret matching\n",
    "def get_strategy(regrets):\n",
    "    strategy = np.zeros(len(regrets))\n",
    "    norm_sum = 0\n",
    "    for i in range(len(regrets)):\n",
    "        if regrets[i] > 0: # only add positive regrets to norm sum\n",
    "            strategy[i] = regrets[i]\n",
    "            norm_sum += strategy[i]\n",
    "    for i in range(len(regrets)):\n",
    "        if norm_sum > 0: # calculate normalized strategy\n",
    "            strategy[i] /= norm_sum\n",
    "        else:\n",
    "            strategy[i] = 1 / len(regrets)\n",
    "            \n",
    "    return strategy\n",
    "\n",
    "# Get an action based on a normalized strategy\n",
    "# strategy = [0.6, 0.2, 0.2]\n",
    "# 60% chance to return 0, 20% chance to return 1 or 2\n",
    "def get_action(strategy):\n",
    "    return np.random.choice(ACTIONS, p=strategy)\n",
    "\n",
    "# Plays a game and updates regrets\n",
    "def play(): \n",
    "    global regrets\n",
    "    global strategy_sum\n",
    "    # get my current strategy\n",
    "    strategy = get_strategy(regrets)\n",
    "    # select an action based on my strategy\n",
    "    my_action = get_action(strategy)\n",
    "    # select an opponent action \n",
    "    opp_action = get_action(strategy)\n",
    "    \n",
    "    # try replacing with this and seeing how it responds\n",
    "#     opp_action = get_action(opp_strategy)\n",
    "    \n",
    "    # initialize utility of each action to be zero\n",
    "    utils = np.zeros(len(ACTIONS))\n",
    "    # get utility of each action\n",
    "    for a in ACTIONS:\n",
    "        utils[a] = get_utility(a, opp_action)\n",
    "    # update regrets through regret matching\n",
    "    for a in ACTIONS:\n",
    "        regrets[a] += utils[a] - utils[my_action]\n",
    "        # increment avg strategy\n",
    "        strategy_sum[a] += strategy[a]\n",
    "\n",
    "# Let's train for 10000 iterations and see if our\n",
    "# strategy converges to a nash equillibrium\n",
    "for i in range(1000):\n",
    "    play()\n",
    "    if i % 100 == 0:\n",
    "        print(get_strategy(strategy_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Hopefully the strategy is getting close to throwing ROCK, PAPER, and SCISSORS 1/3 of the time each.  This turns out to the be Nash Equilibrium strategy.  You can try repacing the opponent strategy with some other fixed strategy and seeing how that effects ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CFRM Example: Simplfied Poker\n",
    "\n",
    "To demonstrate how regret matching is used in CFRM, we'll use a simplified version of poker.  To do this we'll restrict both the **state** and the **action** space.  In our simplified poker game, players are dealt one card.  The first player can then decide to BET 1 chip or CHECK.  If the first player has BET 1 chip, the second player can decide to CALL, FOLD, or RAISE 1 chip.  There will be no reraising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Game Tree\n",
    "\n",
    "Let's build a game tree to reflect this simplified poker game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "BET, RAISE, CHECK, CALL, FOLD = 0, 1, 2, 3, 4\n",
    "ACTIONS = [BET, RAISE, CHECK, CALL, FOLD]\n",
    "\n",
    "# Represents the state of the game when building the tree\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        self.current_player = 0 # 0: first player, 1: second player\n",
    "        self.pot = 1 # size of the pot in chips\n",
    "        self.bets_settled = False # has a player called or folded\n",
    "        self.player_folded = None # which player has folded\n",
    "        self.player_bets = [0, 0]\n",
    "        self.player_cards = [0, 0]\n",
    "        self.hist = \"\"\n",
    "        \n",
    "    def randomize_cards(self):\n",
    "        self.player_cards[0] = np.random.randint(10)\n",
    "        self.player_cards[1] = np.random.randint(10)\n",
    "        while self.player_cards[1] == self.player_cards[0]:\n",
    "            self.player_cards[1] = np.random.randint(10)\n",
    "        \n",
    "    def get_valid_actions(self):\n",
    "        return list(filter(lambda a: self.is_action_valid(a), ACTIONS))\n",
    "    \n",
    "    def get_infoset_key(self):\n",
    "        return str(self.player_cards[self.current_player]) + self.hist\n",
    "    \n",
    "    def is_action_valid(self, action):\n",
    "        if action == CHECK:\n",
    "            # if there is money in the pot, you can't check\n",
    "            # you must call, fold, or raise\n",
    "            return np.sum(self.player_bets) == 0\n",
    "        if action == BET:\n",
    "            # if there is money in the pot, you can't bet\n",
    "            # you must raise\n",
    "            return np.sum(self.player_bets) == 0\n",
    "        if action == CALL:\n",
    "            # if there is no money in the pot, you can't call\n",
    "            return np.sum(self.player_bets) != 0\n",
    "        if action == FOLD:\n",
    "            # if there is no money in the pot, don't fold.  Check instead\n",
    "            return np.sum(self.player_bets) != 0\n",
    "        if action == RAISE:\n",
    "            # Only the second player can raise,\n",
    "            # and the first player has to have alrady bet\n",
    "            return np.sum(self.player_bets) != 0 and self.current_player == 1\n",
    "        return False\n",
    "        \n",
    "    # apply a valid action to the game state\n",
    "    # and return a new, updated state\n",
    "    def apply_action(self, action):\n",
    "        next_state = deepcopy(self)\n",
    "        if action == CHECK:\n",
    "            if next_state.current_player == 0:\n",
    "                next_state.current_player = 1\n",
    "            else:\n",
    "                next_state.bets_settled = True\n",
    "            next_state.hist += \"x\"\n",
    "        if action == BET:\n",
    "            next_state.player_bets[next_state.current_player] += 1\n",
    "            next_state.current_player = 1 - next_state.current_player\n",
    "            next_state.hist += \"b\"\n",
    "        if action == RAISE:\n",
    "            next_state.player_bets[next_state.current_player] += 1\n",
    "            next_state.pot += np.sum(next_state.player_bets)\n",
    "            next_state.player_bets = [0, 0]\n",
    "            next_state.player_bets[next_state.current_player] += 1\n",
    "            next_state.current_player = 1 - next_state.current_player\n",
    "            next_state.hist += \"r\"\n",
    "        if action == FOLD:\n",
    "            next_state.player_folded = next_state.current_player\n",
    "            next_state.bets_settled = True\n",
    "            next_state.hist += 'f'\n",
    "        if action == CALL:\n",
    "            next_state.player_bets[next_state.current_player] += 1\n",
    "            next_state.pot += np.sum(next_state.player_bets)\n",
    "            next_state.bets_settled = True\n",
    "            next_state.hist += 'c'\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, we haven't actually built the game tree, this will be fine for our purposes.  We can traverse game tree by applying actions recursivily.  This is not very efficient and should not be done in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Algorithm\n",
    "\n",
    "Lets actually build our CFRM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\r"
     ]
    }
   ],
   "source": [
    "# First, we need a table to store our regrets\n",
    "# We're going to use a str->arr dictionary where\n",
    "# str represents or information set\n",
    "# for example 'Abc' means that we hold an Ace and the action went BET .. CALL\n",
    "# The value of this array will be regrets as we used before\n",
    "infosets = dict()\n",
    "\n",
    "def get_or_create_infoset(n_actions, key):\n",
    "    global infosets\n",
    "    if not key in infosets:\n",
    "        infosets[key] = {\n",
    "            'regrets': np.ones(n_actions) / n_actions,\n",
    "            'strategy_sum': np.zeros(n_actions)\n",
    "        }\n",
    "    return infosets[key]\n",
    "\n",
    "# Our main function is called cfr\n",
    "# \n",
    "# hist represents the true game state\n",
    "# for example 'ATxx' means player 1 has an ACE player two has a TEN and it went CHECK .. CHECK\n",
    "# \n",
    "# state represents the current state (or node we're in)\n",
    "#\n",
    "# cfr reach is the counterfactual reach probability of reaching the current node\n",
    "# A reach probability is the probability of reaching a current node\n",
    "# For example\n",
    "#    10% chance to be dealt an A\n",
    "#    * 50% chance of opponent checking with a J\n",
    "#    * 50% chance of me raising with an A\n",
    "#    = 2.5% chance of reaching this node\n",
    "# A counterfactual reach probabililty only accounts for things outside of our control\n",
    "# This would mean instead of 2.5% we have a 5% of reaching this node\n",
    "#    10% chance to be dealt an A\n",
    "#    * 50% of opponent checking with a J\n",
    "#    = 5% chance of reaching this node\n",
    "# \n",
    "# The function returns the utility of the current node\n",
    "# it does this by calling cfr recursivily until it reaches a terminal node\n",
    "# \n",
    "def cfr(state, cfr_reach):\n",
    "    global infosets\n",
    "    # if this is a terminal node,\n",
    "    # return the utility for each player\n",
    "    if state.bets_settled:\n",
    "        if not state.player_folded == None:\n",
    "            return (1 if state.player_folded == 1 else -1) * state.pot\n",
    "            # return [state.pot, -state.pot] if state.player_folded == 1 else [-state.pot, state.pot]\n",
    "        # Tie\n",
    "        if state.player_cards[0] == state.player_cards[1]:\n",
    "            #return [0, 0]\n",
    "            return 0\n",
    "        # player 1 wins\n",
    "        elif state.player_cards[0] > state.player_cards[1]:\n",
    "            return state.pot\n",
    "            #return [state.pot, -state.pot]\n",
    "        # player 2 wins\n",
    "        else:\n",
    "            return -state.pot\n",
    "            #return [-state.pot, state.pot]\n",
    "        \n",
    "    player = state.current_player\n",
    "    # get all valid actions we can take\n",
    "    actions = state.get_valid_actions()\n",
    "    # get our infoset & strategy\n",
    "    infoset = get_or_create_infoset(len(actions), state.get_infoset_key())\n",
    "    strategy = get_strategy(infoset['regrets'])\n",
    "    utils = np.zeros(len(actions))\n",
    "    node_util = 0\n",
    "    # calculate utils\n",
    "    for a in range(len(actions)):\n",
    "        # calculate new reach probabilty\n",
    "        child_cfr_reach = cfr_reach.copy()\n",
    "        child_cfr_reach[1 - player] *= strategy[a]\n",
    "        utils[a] = cfr(state.apply_action(actions[a]), child_cfr_reach)\n",
    "        # cummulate utility of action * weighted by likelihood of choosing this action\n",
    "        node_util += strategy[a] * utils[a]\n",
    "    # update regrets\n",
    "    for a in range(len(actions)):\n",
    "        # Probability of us reaching this node * the regret difference\n",
    "        infoset['regrets'][a] += cfr_reach[player] * (utils[a] - node_util) * (1 if player == 0 else -1)\n",
    "        # Cummulate regret\n",
    "        infoset['strategy_sum'][a] += cfr_reach[player] * strategy[a]\n",
    "    return node_util\n",
    "        \n",
    "\n",
    "# Train our strategy\n",
    "def train():\n",
    "    for i in range(10000):\n",
    "        state = GameState()\n",
    "        state.randomize_cards()\n",
    "        cfr(state, [1, 1])\n",
    "        if i % 1000 == 0:\n",
    "            print(i, end='\\r')\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BET', 'CHECK']\n",
      "7 [0.20015026 0.79984974]\n",
      "7 [15.26250723 -6.10720485]\n",
      "3 [0.01600477 0.98399523]\n",
      "3 [-221.60846246    4.55631002]\n",
      "9 [0.89402326 0.10597674]\n",
      "9 [  10.35454392 -124.74410039]\n",
      "4 [0.01432653 0.98567347]\n",
      "4 [-361.63736909    4.29744351]\n",
      "5 [0.00163053 0.99836947]\n",
      "5 [-215.69086083    2.13345121]\n",
      "2 [8.15034829e-04 9.99184965e-01]\n",
      "2 [-114.50458546    2.04972141]\n",
      "0 [0.50091737 0.49908263]\n",
      "0 [ 5.19452353 28.64189144]\n",
      "8 [0.30422569 0.69577431]\n",
      "8 [24.4010326   1.49193866]\n",
      "1 [0.1702434 0.8297566]\n",
      "1 [-34.94750324  15.5775208 ]\n",
      "6 [4.73933649e-04 9.99526066e-01]\n",
      "6 [-50.71625676   1.5       ]\n"
     ]
    }
   ],
   "source": [
    "def action_to_str(action):\n",
    "    if action == BET: return 'BET'\n",
    "    if action == CALL: return 'CALL'\n",
    "    if action == FOLD: return 'FOLD'\n",
    "    if action == CHECK: return 'CHECK'\n",
    "    if action == RAISE: return 'RAISE'\n",
    "    \n",
    "\n",
    "# Lets view the root action node\n",
    "state = GameState()\n",
    "# state = state.apply_action(BET)\n",
    "\n",
    "actions = state.get_valid_actions()\n",
    "\n",
    "print([action_to_str(a) for a in actions])\n",
    "for key in infosets:\n",
    "    if key[1:] == '':\n",
    "        print(key, get_strategy(infosets[key]['strategy_sum']))\n",
    "        print(key, infosets[key]['regrets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
